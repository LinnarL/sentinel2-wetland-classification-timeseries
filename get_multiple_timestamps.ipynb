{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import stackstac\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import xrspatial.multispectral as ms\n",
    "import dask.array as da\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from urllib3.util.retry import Retry\n",
    "from dask.diagnostics import ProgressBar\n",
    "import xarray as xr\n",
    "import bottleneck\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import rioxarray\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: working_dir\n",
      "Created directory: working_dir/labels\n",
      "Directory already exists: working_dir/prediction_maps\n",
      "Directory already exists: working_dir/study_area\n"
     ]
    }
   ],
   "source": [
    "# Set variables\n",
    "local = False # This controls whether or not a cluster is created using Coiled as the provisioning service, for testing a local cluster is recommended.\n",
    "resolution = 10 # This controls the spatial resolution of the fetched datasets - this is changed dynamically using resampling by StackSTAC\n",
    "bands = ['B04', 'B03', 'B02','B07', 'B08', 'SCL'] # This controls the bands of the input data to fetch, it can be useful to limit this to just one band + SCL for testing purposes. SCL is always needed, but the others can be picked as you choose (see Sentinel 2 documentation for band names and descriptions.)\n",
    "\n",
    "# Set search parameters\n",
    "months = ['june', 'july', 'august'] # The months per year to search for data within\n",
    "years = ['2018','2019','2020','2021', '2022', '2023'] # The years to iterate through, processing one median over the above months per year\n",
    "geojson_path = r\"./working_dir/study_area/Arvidsjaur.geojson\" # Point this to where your study area is located, should be a GeoJSON Polygon file. \n",
    "working_dir = \"./working_dir/\" # Point this to a folder named working_dir in the same directory as this script (you might need to create it)\n",
    "max_items = 10 # This limits the amount of items returned from the STAC catalog after filtering - Useful to set to a low value while testing a workflow over and over again.\n",
    "tile_max_cloud = 20 # This limits the included items based on their cloud_percentage parameter. This value is however tile wide!! I.e this statistic has been calculated on a whole Sentinel 2 tile, which is 100x100km and thus not neccesarily applicable for the area of interest.\n",
    "\n",
    "\n",
    "# Define the directory structure\n",
    "directories = [\n",
    "    'working_dir',\n",
    "    'working_dir/labels',\n",
    "    'working_dir/prediction_maps',\n",
    "    'working_dir/study_area'\n",
    "]\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Created directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters and initialize pystac + coiled\n",
    "# The retry parameter is used for accessing the STAC catalog, as recommmended by Microsoft.\n",
    "\n",
    "retry = Retry(\n",
    "    total=5, backoff_factor=1, status_forcelist=[502, 503, 504], allowed_methods=None\n",
    ")\n",
    "pystac_client.stac_api_io.StacApiIO(max_retries=retry)\n",
    "\n",
    "\n",
    "# If the local parameter is set to true, skip importing Coiled and just make a cluster on this local machine. Workers is automatically equal to the number of cores of your processor in that case.\n",
    "if local:\n",
    "    cluster = LocalCluster(name = \"Linnars-Dator\")\n",
    "    client = Client(cluster)\n",
    "else:\n",
    "    import coiled # See the documentation available for Coiled on how to set up Coiled + Dask!\n",
    "    cluster = coiled.Cluster(name=\"WetlandsClassification\", shutdown_on_close=True)\n",
    "    cluster.adapt(n_workers = 1, maximum=10)\n",
    "    client = cluster.get_client()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell converts the GeoJSON to a suitable geometry for searching the STAC catalog. If the GeoJSON has multiple polygons, the first one is used. Either Polygons or MultiPolygons can be used.\n",
    "\n",
    "# Get search bbox from GeoJSON\n",
    "import json\n",
    "\n",
    "# After loading the GeoJSON file\n",
    "with open(geojson_path) as f:\n",
    "    region = json.load(f)\n",
    "\n",
    "# Extract the geometry from the FeatureCollection\n",
    "if region['type'] == 'FeatureCollection':\n",
    "    # Get the first feature's geometry\n",
    "    geometry = region['features'][0]['geometry']\n",
    "    \n",
    "    # If it's a MultiPolygon, it's already in the correct format\n",
    "    # If it's a single Polygon, it will also work\n",
    "    if geometry['type'] in ['MultiPolygon', 'Polygon']:\n",
    "        search_geometry = geometry\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported geometry type: {geometry['type']}\")\n",
    "else:\n",
    "    search_geometry = region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This gets the extents of the area polygon above, which is the actual area used to search for data.\n",
    "\n",
    "def get_bbox(geometry):\n",
    "    if geometry['type'] == 'Polygon':\n",
    "        coords = np.array(geometry['coordinates'][0])\n",
    "        return [coords[:, 0].min(), coords[:, 1].min(), coords[:, 0].max(), coords[:, 1].max()]\n",
    "    elif geometry['type'] == 'MultiPolygon':\n",
    "        all_coords = []\n",
    "        for polygon in geometry['coordinates']:\n",
    "            all_coords.extend(polygon[0])\n",
    "        coords = np.array(all_coords)\n",
    "        return [coords[:, 0].min(), coords[:, 1].min(), coords[:, 0].max(), coords[:, 1].max()]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported geometry type: {geometry['type']}\")\n",
    "\n",
    "# Calculate and print the bounding box\n",
    "bbox = get_bbox(search_geometry)\n",
    "print(\"Bounding box coordinates (minx, miny, maxx, maxy):\", bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of datetime ranges\n",
    "datetime_ranges = []\n",
    "for year in years:\n",
    "    # Get first and last month from the months list\n",
    "    first_month = months[0]\n",
    "    last_month = months[-1]\n",
    "    \n",
    "    # Convert month names to numbers (1-12)\n",
    "    month_to_num = {\n",
    "        'january': '01', 'february': '02', 'march': '03', 'april': '04',\n",
    "        'may': '05', 'june': '06', 'july': '07', 'august': '08',\n",
    "        'september': '09', 'october': '10', 'november': '11', 'december': '12'\n",
    "    }\n",
    "    \n",
    "    # Convert month names to numbers and ensure they're zero-padded\n",
    "    first_month = month_to_num[first_month.lower()]\n",
    "    last_month = month_to_num[last_month.lower()]\n",
    "    \n",
    "    datetime_ranges.append(f\"{year}-{first_month}-01/{year}-{last_month}-31\")\n",
    "\n",
    "datetime_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sometimes reading a dataset will fail - usually when processing times are long (over 30min per datetime)\n",
    "# I think this is to do with the access to the PySTAC items timing out. \n",
    "# This needs some form of error handling, but I have been working around the error through\n",
    "# just rerunning the script and removing the already fetched years from the list of years above.\n",
    "for datetime in datetime_ranges:\n",
    "\n",
    "    catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "    # Create an itemcollection with all the selected datetime ranges\n",
    "    search = catalog.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    intersects = search_geometry,\n",
    "    datetime=datetime,\n",
    "    max_items = max_items,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": tile_max_cloud}}\n",
    "    )\n",
    "    items = search.item_collection()\n",
    "    print(f\"\\nFound {len(items)} items for range {datetime}\")\n",
    "\n",
    "    # Stack itemcollection\n",
    "    data = (\n",
    "        stackstac.stack(\n",
    "            items,\n",
    "            assets=bands,  # The selected bands from the assets list \n",
    "            resolution=resolution,\n",
    "            epsg=3006,\n",
    "            chunksize= (-1, 1, 128, 128),\n",
    "            dtype=np.dtype('float'),\n",
    "            bounds_latlon = bbox\n",
    "        )\n",
    "        .where(lambda x: x > 0, other=np.nan) \n",
    "    )\n",
    "\n",
    "    # After creating your dask array\n",
    "    # Learning about how Dask data is chunked is useful - chunksize needs to be big enough to get good parallelization efficiency, but small enough to fit in memory. A size of around 100MB chunks is generally a good aim. \n",
    "    print(\"Array size information:\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Size in bytes: {data.data.nbytes}\")\n",
    "    print(f\"Size in GB: {data.data.nbytes / 1e9:.2f} GB\")\n",
    "    print(f\"Number of chunks: {data.data.npartitions}\"))\n",
    "\n",
    "    # Filter out pixels in all bands by the corresponding SCL classification. The classes to include are according to their pixel value, see Sentinel 2 SCL band documentation.\n",
    "    scl_band_name = 'SCL'\n",
    "    clear_classes = [4,5,6,7,11]\n",
    "\n",
    "    try:\n",
    "        data_scl = data.sel(band=scl_band_name)\n",
    "    except KeyError:\n",
    "        print(f\"Error: SCL band names '{scl_band_name}' was not found in data.coords['band']. Available bands: {list(data.band.values)}\")\n",
    "\n",
    "    # Create mask for clear pixels\n",
    "    clear_mask = data_scl.isin(clear_classes)\n",
    "\n",
    "    # Apply mask to all bands except SCL\n",
    "    data = data.where(clear_mask)\n",
    "\n",
    "    # Compute median for the year\n",
    "    data_median = data.median(dim='time', skipna=True).compute()\n",
    "\n",
    "\n",
    "    # Write the dataset to a GeoTIFF file with timestamp\n",
    "    output_path = f\"./working_dir/median_pixels_{datetime.replace('/', '_')}.tif\"\n",
    "    data_median.rio.to_raster(output_path, driver=\"GTiff\", compress = \"LZW\")\n",
    "    print(f\"Saved dataset to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
